{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1AoYYvmZQUU3-ITo9DotSXD51nwmgO2SL",
      "authorship_tag": "ABX9TyNhqxjKbVqU6nKri6iJjzN+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rehanali455/rehanali455/blob/main/21_Rehan_NLP_Assigment_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXPLANATION:**\n",
        "\n",
        "Import Libraries: Load required tools and download NLTK resources like tokenizers and stopwords.\n",
        "\n"
      ],
      "metadata": {
        "id": "B5kucdlzVMoK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "g8UWLMcf4C36"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This downloads the Punkt tokenizer models, required for splitting text into sentences or words.\n",
        "\n"
      ],
      "metadata": {
        "id": "WQ4eKPwNVZlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK datasets\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UfoHSrO5B8m",
        "outputId": "c8b2d6bf-7a57-40ca-e802-06cd0942b6d4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read Text File: Open sample.txt and read its content for processing.\n",
        "\n"
      ],
      "metadata": {
        "id": "CZamCSLeVqZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Read a sample text file (ensure a file named 'sample.txt' exists with 1000+ words)\n",
        "with open('sample.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()"
      ],
      "metadata": {
        "id": "lWZhkSEe5E7G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the initial part of the input text"
      ],
      "metadata": {
        "id": "AwcHK670Wa6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the initial part of the input text\n",
        "print(\"Sample input text:\", text[:500], \"\\n...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pG034Btn5HG5",
        "outputId": "fb96c518-09a7-48d9-8c68-0fa408f2d267"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample input text: \n",
            "In the vast expanse of the digital age, information flows ceaselessly, shaping the contours of our understanding and interaction with the world.\n",
            "Technology has become the heartbeat of society, influencing every facet of our existence—from communication and commerce to education and entertainment.\n",
            "As we navigate this ever-evolving landscape, the importance of critical thinking and digital literacy cannot be overstated.\n",
            "\n",
            "The history of technology is a testament to human ingenuity and resilience.  \n",
            "...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize Text: Convert text to lowercase and remove punctuation.\n",
        "\n"
      ],
      "metadata": {
        "id": "QlQ2awjeWe7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Text normalization\n",
        "# Lowercasing and removing punctuation\n",
        "normalized_text = text.lower()\n",
        "normalized_text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", normalized_text)\n",
        "print(\"\\nNormalized text sample:\", normalized_text[:500], \"\\n...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEdJt2vJ5Nc1",
        "outputId": "d2e2bf2e-afc1-446c-f2f7-8380c6a80de6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Normalized text sample: \n",
            "in the vast expanse of the digital age information flows ceaselessly shaping the contours of our understanding and interaction with the world\n",
            "technology has become the heartbeat of society influencing every facet of our existence—from communication and commerce to education and entertainment\n",
            "as we navigate this everevolving landscape the importance of critical thinking and digital literacy cannot be overstated\n",
            "\n",
            "the history of technology is a testament to human ingenuity and resilience from the  \n",
            "...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize Text: Split the text into individual words (tokens).\n",
        "\n"
      ],
      "metadata": {
        "id": "mu7g8CL4WsgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Tokenization\n",
        "tokens = word_tokenize(normalized_text)\n",
        "print(\"\\nTokenized sample:\", tokens[:30], \"\\n...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p19RwjL45QQf",
        "outputId": "03e9dabe-c2ee-48d6-895b-a00a04eeb821"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenized sample: ['in', 'the', 'vast', 'expanse', 'of', 'the', 'digital', 'age', 'information', 'flows', 'ceaselessly', 'shaping', 'the', 'contours', 'of', 'our', 'understanding', 'and', 'interaction', 'with', 'the', 'world', 'technology', 'has', 'become', 'the', 'heartbeat', 'of', 'society', 'influencing'] \n",
            "...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove Stopwords: Remove common words like \"the\", \"is\", \"and\" to focus on important words.\n",
        "\n"
      ],
      "metadata": {
        "id": "FKRYiPLYWDBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Stopword removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens_without_stopwords = [token for token in tokens if token not in stop_words]\n",
        "print(\"\\nSample after stopword removal:\", tokens_without_stopwords[:30], \"\\n...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WX4lQc05SwL",
        "outputId": "0f863d7d-b3b0-41b0-8642-21a2dd1631ff"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample after stopword removal: ['vast', 'expanse', 'digital', 'age', 'information', 'flows', 'ceaselessly', 'shaping', 'contours', 'understanding', 'interaction', 'world', 'technology', 'become', 'heartbeat', 'society', 'influencing', 'every', 'facet', 'existence—from', 'communication', 'commerce', 'education', 'entertainment', 'navigate', 'everevolving', 'landscape', 'importance', 'critical', 'thinking'] \n",
            "...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import Stemming and Lemmatization Funtions"
      ],
      "metadata": {
        "id": "edDWK55NW2Jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Stemming and Lemmatization\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "09_LJEpv5U2S"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stem Words: Reduce words to their root form (e.g., \"running\" → \"run\").\n"
      ],
      "metadata": {
        "id": "4UsxF6GgXJgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply stemming\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens_without_stopwords]\n",
        "print(\"\\nStemmed tokens sample:\", stemmed_tokens[:30], \"\\n...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "th2aATIn5Xfp",
        "outputId": "7155618a-14e3-4ec0-99c2-d487a49c2d00"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemmed tokens sample: ['vast', 'expans', 'digit', 'age', 'inform', 'flow', 'ceaselessli', 'shape', 'contour', 'understand', 'interact', 'world', 'technolog', 'becom', 'heartbeat', 'societi', 'influenc', 'everi', 'facet', 'existence—from', 'commun', 'commerc', 'educ', 'entertain', 'navig', 'everevolv', 'landscap', 'import', 'critic', 'think'] \n",
            "...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Lemmatize Words: Find the base form of words considering meaning (e.g., \"better\" → \"good\").\n"
      ],
      "metadata": {
        "id": "OzJTk7sIXNlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply lemmatization\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens_without_stopwords]\n",
        "print(\"\\nLemmatized tokens sample:\", lemmatized_tokens[:30], \"\\n...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYeqr7Jc5Z30",
        "outputId": "323e4c8d-bbf2-46ef-8ee1-1080089dd61b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatized tokens sample: ['vast', 'expanse', 'digital', 'age', 'information', 'flow', 'ceaselessly', 'shaping', 'contour', 'understanding', 'interaction', 'world', 'technology', 'become', 'heartbeat', 'society', 'influencing', 'every', 'facet', 'existence—from', 'communication', 'commerce', 'education', 'entertainment', 'navigate', 'everevolving', 'landscape', 'importance', 'critical', 'thinking'] \n",
            "...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Combine Tokens: Join processed words back into a single text string.\n"
      ],
      "metadata": {
        "id": "blEvgYj9XQjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Summary output\n",
        "processed_text = ' '.join(lemmatized_tokens)\n",
        "print(\"\\nProcessed text sample:\", processed_text[:500], \"\\n...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBJ0o0I95cAe",
        "outputId": "1bb58455-5d50-4a7d-ab40-dbdd0cc63bb4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processed text sample: vast expanse digital age information flow ceaselessly shaping contour understanding interaction world technology become heartbeat society influencing every facet existence—from communication commerce education entertainment navigate everevolving landscape importance critical thinking digital literacy overstated history technology testament human ingenuity resilience wheel microchip invention built upon knowledge past creating tapestry progress innovation early day computing pioneer like ada love \n",
            "...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Save Output: Write the cleaned text to processed_output.txt."
      ],
      "metadata": {
        "id": "5hJP_A_QXVIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the processed text to an output file\n",
        "with open('processed_output.txt', 'w', encoding='utf-8') as out_file:\n",
        "    out_file.write(processed_text)"
      ],
      "metadata": {
        "id": "pLFntByP5eLT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nText preprocessing complete. Processed text saved to 'processed_output.txt'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOv3N5wM5gtV",
        "outputId": "553398c9-c26c-4132-8932-00e1eae1897e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Text preprocessing complete. Processed text saved to 'processed_output.txt'\n"
          ]
        }
      ]
    }
  ]
}